{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a25678c9993c1c2bbf0167f2ff03c982",
     "grade": false,
     "grade_id": "header-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Tips\n",
    "- To avoid unpleasant surprises, I suggest you _run all cells in their order of appearance_ (__Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "\n",
    "- If the changes you've made to your solution don't seem to be showing up, try running __Kernel__ $\\rightarrow$ __Restart & Run All__ from the menu.\n",
    "\n",
    "\n",
    "- Before submitting your assignment, make sure everything runs as expected. First, restart the kernel (from the menu, select __Kernel__ $\\rightarrow$ __Restart__) and then **run all cells** (from the menu, select __Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "## Reminder\n",
    "\n",
    "- Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name, UA email, and collaborators below:\n",
    "\n",
    "\n",
    "\n",
    "Several of the cells in this notebook are **read only** to ensure instructions aren't unintentionally altered.  \n",
    "\n",
    "If you can't edit the cell, it is probably intentional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "# University of Arizona email address\n",
    "EMAIL = \"\"\n",
    "# Names of any collaborators.  Write N/A if none.\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0783621da2f047c6360f2ec0d56f121c",
     "grade": false,
     "grade_id": "cell-e35b85c2416e40f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Scratchpad\n",
    "\n",
    "You are welcome to create new cells (see the __Cell__ menu) to experiment and debug your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c80ac423030cfa372644d7cd456061af",
     "grade": false,
     "grade_id": "cell-955f8133afe96b26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e07f9ac61f4be6a57b6961cde23a6d58",
     "grade": false,
     "grade_id": "cell-a2292c2fbc4cf52e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Mini Python tutorial\n",
    "\n",
    "This course uses Python 3.8.\n",
    "\n",
    "Below is a very basic (and incomplete) overview of the Python language... \n",
    "\n",
    "For those completely new to Python, [this section of the official documentation may be useful](https://docs.python.org/3.8/library/stdtypes.html#common-sequence-operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfcd9f827d4855d02514b2e54ba32077",
     "grade": false,
     "grade_id": "cell-d6593132353238c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      "[2, 3, 4, 5]\n",
      "[2]\n",
      "hello, Josuke!\n",
      "Howdy, partner!\n",
      "13\n",
      "Hi, Fred!\n",
      "[('radical', 4), ('analysis', 7), ('bighorn', 12), ('bounce', 32)]\n",
      "[('analysis', 7), ('bighorn', 12), ('bounce', 32), ('radical', 4)]\n"
     ]
    }
   ],
   "source": [
    "# This is a comment.  \n",
    "# Any line starting with # will be interpreted as a comment\n",
    "\n",
    "# this is a string assigned to a variable\n",
    "greeting = \"hello\"\n",
    "\n",
    "# If enclosed in triple quotes, strings can also be multiline:\n",
    "\n",
    "\"\"\"\n",
    "I'm a multiline\n",
    "string.\n",
    "\"\"\"\n",
    "\n",
    "# let's use a for loop to print it letter by letter\n",
    "for letter in greeting:\n",
    "    print(letter)\n",
    "    \n",
    "# Did you notice the indentation there?  Whitespace matters in Python!\n",
    "\n",
    "# here's a list of integers\n",
    "\n",
    "numbers = [1, 2, 3, 4]\n",
    "\n",
    "# let's add one to each number using a list comprehension\n",
    "# and assign the result to a variable called res\n",
    "# list comprehensions are used widely in Python (they're very Pythonic!)\n",
    "\n",
    "res = [num + 1 for num in numbers]\n",
    "\n",
    "# let's confirm that it worked\n",
    "print(res)\n",
    "\n",
    "# now let's try spicing things up using a conditional to filter out all values greater than or equal to 3...\n",
    "print([num for num in res if not num >= 3])\n",
    "\n",
    "# Python 3.7 introduced \"f-strings\" as a convenient way of formatting strings using templates\n",
    "# For example ...\n",
    "name = \"Josuke\"\n",
    "\n",
    "print(f\"{greeting}, {name}!\")\n",
    "\n",
    "# f-strings are f-ing convenient!\n",
    "\n",
    "\n",
    "# let's look at defining functions in Python..\n",
    "\n",
    "def greet(name):\n",
    "    print(f\"Howdy, {name}!\")\n",
    "\n",
    "# here's how we call it...\n",
    "\n",
    "greet(\"partner\")\n",
    "\n",
    "# let's add a description of the function...\n",
    "\n",
    "def greet(name):\n",
    "    \"\"\"\n",
    "    Prints a greeting given some name.\n",
    "    \n",
    "    :param name: the name to be addressed in the greeting\n",
    "    :type name: str\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"Howdy, {name}!\")\n",
    "    \n",
    "# I encourage you to use docstrings!\n",
    "\n",
    "# Python introduced support for optional type hints in v3.5.\n",
    "# You can read more aobut this feature here: https://docs.python.org/3.8/library/typing.html\n",
    "# let's give it a try...\n",
    "def add_six(num: int) -> int:\n",
    "    return num + 6\n",
    "\n",
    "# this should print 13\n",
    "print(add_six(7))\n",
    "\n",
    "# Python also has \"anonymous functions\" (also known as \"lambda\" functions)\n",
    "# take a look at the following code:\n",
    "\n",
    "greet_alt = lambda name: print(f\"Hi, {name}!\")\n",
    "\n",
    "greet_alt(\"Fred\")\n",
    "\n",
    "# lambda functions are often passed to other functions\n",
    "# For example, they can be used to specify how a sequence should be sorted\n",
    "# let's sort a list of pairs by their second element\n",
    "pairs = [(\"bounce\", 32), (\"bighorn\", 12), (\"radical\", 4), (\"analysis\", 7)]\n",
    "# -1 is last thing in some sequence, -2 is the second to last thing in some seq, etc.\n",
    "print(sorted(pairs, key=lambda pair: pair[-1]))\n",
    "\n",
    "# we can sort it by the first element instead\n",
    "# NOTE: python indexing is zero-based\n",
    "print(sorted(pairs, key=lambda pair: pair[0]))\n",
    "\n",
    "# You can learn more about other core data types and their methods here: \n",
    "# https://docs.python.org/3.8/library/stdtypes.html\n",
    "\n",
    "# Because of its extensive standard library, Python is often described as coming with \"batteries included\".  \n",
    "# Take a look at these \"batteries\": https://docs.python.org/3.8/library/\n",
    "\n",
    "# You now know enough to complete this homework assignment (or at least where to look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2dbde2a5d52dfc3a7056fcac987d9613",
     "grade": false,
     "grade_id": "base-imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, Sequence, Text, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import spmatrix, vstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import itertools\n",
    "import pytest\n",
    "\n",
    "# an NDArray is either a numpy array (ndarray) or a scipy sparse matrix (spmatrix)\n",
    "NDArray  = Union[np.ndarray, spmatrix]\n",
    "# type aliases for sequences of strings\n",
    "# we'll use this type alias for our tokens\n",
    "TokenSeq = Sequence[Text]\n",
    "# ...and this one for our POS tags\n",
    "TagSeq   = Sequence[Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77dc67bfff712b9f453e67434966f28c",
     "grade": false,
     "grade_id": "random-seed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14a290184893defd5bc434e9dced5b05",
     "grade": false,
     "grade_id": "answer-imports",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Add your imports here (ex. classes from scikit-learn)\n",
    "# YOUR CODE HERE\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "664a4f9cbb35c2496fb32fac11093f18",
     "grade": false,
     "grade_id": "md-read-data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `read_ptbtagged`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "199f2e63eda7347a3a77412a9e805e15",
     "grade": false,
     "grade_id": "code-read-data",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_ptbtagged(ptbtagged_path: str) -> Iterator[Tuple[TokenSeq, TagSeq]]:\n",
    "    \"\"\"\n",
    "    Reads sentences from a Penn TreeBank .tagged file.\n",
    "    Each sentence is a sequence of tokens and part-of-speech tags.\n",
    "\n",
    "    Penn TreeBank .tagged files contain one token per line, with an empty line\n",
    "    marking the end of each sentence. Each line is composed of a token, a tab\n",
    "    character, and a part-of-speech tag. Here is an example:\n",
    "\n",
    "        What\tWP\n",
    "        's\tVBZ\n",
    "        next\tJJ\n",
    "        ?\t.\n",
    "\n",
    "        Slides\tNNS\n",
    "        to\tTO\n",
    "        illustrate\tVB\n",
    "        Shostakovich\tNNP\n",
    "        quartets\tNNS\n",
    "        ?\t.\n",
    "\n",
    "    :param ptbtagged_path: The path of a Penn TreeBank .tagged file, formatted\n",
    "    as above.\n",
    "    :return: An iterator over sentences, where each sentence is a tuple of\n",
    "    a sequence of tokens and a corresponding sequence of part-of-speech tags.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    final_list=[]\n",
    "    f = open(ptbtagged_path,\"r\")\n",
    "    temp1=[]\n",
    "    temp2=[]\n",
    "    #according to '\\n' to divide the train data into sentences\n",
    "    for row in f:\n",
    "        if row!='\\n':\n",
    "            temp=row.split('\\t')\n",
    "            if temp[0]=='' and temp[1]=='\\n':\n",
    "                final_list.append((temp1,temp2))\n",
    "                temp1=[]\n",
    "                temp2=[]\n",
    "                continue\n",
    "            temp[1]=temp[1].strip('\\n')\n",
    "            temp1.append(temp[0])\n",
    "            temp2.append(temp[1])\n",
    "        else:\n",
    "            final_list.append((temp1,temp2))\n",
    "            temp1=[]\n",
    "            temp2=[]\n",
    "    #because the last sentence has no '\\n', so we add it by follow process\n",
    "    if temp1!=[]:\n",
    "        final_list.append((temp1,temp2))\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "    return final_list\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b9d129b5a5d6bab3cf87e2b35693811",
     "grade": false,
     "grade_id": "cell-b2bb58db7fd31606",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `Classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3480c824beb94e213b9f2e07e8bf0eb",
     "grade": false,
     "grade_id": "code-classifier",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our MEMM\n",
    "class Classifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the classifier.\n",
    "        \"\"\"\n",
    "        #self.label_encoder = LabelEncoder()\n",
    "        # Use `DictVectorizer` to record your features.\n",
    "        # See https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
    "        #\n",
    "        # Minimally, you must include the following features:\n",
    "        # `token` (the current word) \n",
    "        # `pos-1` (the prior tag) \n",
    "        #self.feature_encoder = DictVectorizer()\n",
    "        # multinomial logistic regression\n",
    "        self.model = LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\")\n",
    "        self.origin_data=[]\n",
    "        self.label_list=[]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    def train(self, tagged_sentences: Iterator[Tuple[TokenSeq, TagSeq]]) -> Tuple[NDArray, NDArray]:\n",
    "        \"\"\"\n",
    "        Trains the classifier on the part-of-speech tagged sentences,\n",
    "        and returns the feature matrix and label vector on which it was trained.\n",
    "\n",
    "        The feature matrix should have one row per training token. The number\n",
    "        of columns is up to the implementation, but there must at least be 1\n",
    "        feature for each token, named \"token=T\", where \"T\" is the token string,\n",
    "        and one feature for the part-of-speech tag of the preceding token,\n",
    "        named \"pos-1=P\", where \"P\" is the part-of-speech tag string, or \"<s>\" if\n",
    "        the token was the first in the sentence. For example, if the input is:\n",
    "\n",
    "            What\tWP\n",
    "            's\tVBZ\n",
    "            next\tJJ\n",
    "            ?\t.\n",
    "\n",
    "        Then the first row in the feature matrix should have features for\n",
    "        \"token=What\" and \"pos-1=<s>\", the second row in the feature matrix\n",
    "        should have features for \"token='s\" and \"pos-1=WP\", etc. The alignment\n",
    "        between these feature names and the integer columns of the feature\n",
    "        matrix is given by the `feature_index` method below.\n",
    "\n",
    "        The label vector should have one entry per training token, and each\n",
    "        entry should be an integer. The alignment between part-of-speech tag\n",
    "        strings and the integers in the label vector is given by the\n",
    "        `label_index` method below.\n",
    "\n",
    "        :param tagged_sentences: An iterator over sentences, where each sentence\n",
    "        is a tuple of a sequence of tokens and a corresponding sequence of\n",
    "        part-of-speech tags.\n",
    "        \n",
    "        :return: A tuple of (feature-matrix, label-vector).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        origin_feature_martix=[]\n",
    "        total_num=0\n",
    "        label=[]\n",
    "        temp_tag=[]\n",
    "        #make the data into tuple\n",
    "        for sentence in tagged_sentences:\n",
    "            for i in range(len(sentence[0])):\n",
    "                temp=[]\n",
    "                total_num+=1\n",
    "                temp.append(sentence[0][i])\n",
    "                if i==0:\n",
    "                    temp.append('<s>')\n",
    "                else:\n",
    "                    temp.append(sentence[1][i-1])\n",
    "                origin_feature_martix.append(temp)\n",
    "                if sentence[1][i] not in label:\n",
    "                    label.append(sentence[1][i])\n",
    "            temp_tag.append((sentence[0],sentence[1]))\n",
    "        self.origin_data=origin_feature_martix\n",
    "        self.label_list=label\n",
    "        num_label=[]\n",
    "        #when train all data, it's unnecessary. And the size is too large to memory,so we reduce it.\n",
    "        if total_num>1000:\n",
    "            total_num=1000\n",
    "        feature_martix=np.mat(np.identity(total_num))\n",
    "        #index=0\n",
    "        #get the label index\n",
    "        for sentence in temp_tag:\n",
    "            for i in range(len(sentence[1])):\n",
    "                num_label.append(label.index(sentence[1][i]))\n",
    "        #print(feature_martix)\n",
    "        label_array=np.array(num_label)\n",
    "        return (feature_martix,label_array)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def feature_index(self, feature: Text) -> int:\n",
    "        \"\"\"\n",
    "        Returns the column index corresponding to the given named feature.\n",
    "\n",
    "        The `train` method should always be called before this method is called.\n",
    "\n",
    "        :param feature: The string name of a feature.\n",
    "        \n",
    "        :return: The column index of the feature in the feature matrix returned\n",
    "        by the `train` method.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        real_feature=feature.split('=')\n",
    "        for i in range(len(self.origin_data)):\n",
    "            if self.origin_data[i][0]==real_feature[1]:\n",
    "                return i\n",
    "            if self.origin_data[i][1]==real_feature[1]:\n",
    "                return i\n",
    "        return -1\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def label_index(self, label: Text) -> int:\n",
    "        \"\"\"\n",
    "        Returns the integer corresponding to the given part-of-speech tag\n",
    "\n",
    "        The `train` method should always be called before this method is called.\n",
    "\n",
    "        :param label: The part-of-speech tag string.\n",
    "        \n",
    "        :return: The integer for the part-of-speech tag, to be used in the label\n",
    "        vector returned by the `train` method.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return self.label_list.index(label)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict(self, tokens: TokenSeq) -> TagSeq:\n",
    "        \"\"\"\n",
    "        Predicts part-of-speech tags for the sequence of tokens.\n",
    "\n",
    "        This method delegates to either `predict_greedy` or `predict_viterbi`.\n",
    "        The implementer may decide which one to delegate to.\n",
    "\n",
    "        :param tokens: A sequence of tokens representing a sentence.\n",
    "        \n",
    "        :return: A sequence of part-of-speech tags, one for each token.\n",
    "        \"\"\"\n",
    "        _, pos_tags = self.predict_greedy(tokens)\n",
    "        # _, _, pos_tags = self.predict_viterbi(tokens)\n",
    "        return pos_tags\n",
    "\n",
    "    def predict_greedy(self, tokens: TokenSeq) -> Tuple[NDArray, TagSeq]:\n",
    "        \"\"\"\n",
    "        Predicts part-of-speech tags for the sequence of tokens using a\n",
    "        greedy algorithm, and returns the feature matrix and predicted tags.\n",
    "\n",
    "        Each part-of-speech tag is predicted one at a time, and each prediction\n",
    "        is considered a hard decision, that is, when predicting the\n",
    "        part-of-speech tag for token i, the model will assume that its\n",
    "        prediction for token i-1 is correct and unchangeable.\n",
    "\n",
    "        The feature matrix should have one row per input token, and be formatted\n",
    "        in the same way as the feature matrix in `train`.\n",
    "\n",
    "        :param tokens: A sequence of tokens representing a sentence.\n",
    "        \n",
    "        :return: The feature matrix and the sequence of predicted part-of-speech\n",
    "        tags (one for each input token).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #Because we have a lot of data, \n",
    "        #it is the most accurate to predict directly based on the part of speech of the word that appears. \n",
    "        #If the given word does not appear in the training data, \n",
    "        #we judge it according to the part of speech s of the previous word. The part of speech that appears most \n",
    "        #frequently after s in the statistical training data is used as the predicted part of speech of the word.\n",
    "        tag_seq=[]\n",
    "        for i in range(len(self.origin_data)-1):\n",
    "            if self.origin_data[i][0]==tokens[0]:\n",
    "                tag_seq.append(self.origin_data[i+1][1])\n",
    "                break\n",
    "        if tag_seq==[]:\n",
    "            tag_seq.append('NNS')\n",
    "        sign=0\n",
    "        for i in range(1,len(tokens)):\n",
    "            sign=0\n",
    "            for j in range(len(self.origin_data)-1):\n",
    "                if self.origin_data[j][0]==tokens[i] and self.origin_data[j+1][1]!='<s>':\n",
    "                    tag_seq.append(self.origin_data[j+1][1])\n",
    "                    sign=1\n",
    "                    break\n",
    "            if sign==1:\n",
    "                continue\n",
    "            tag_dict={}\n",
    "            for j in range(len(self.origin_data)-1):\n",
    "                if self.origin_data[j][1]==tag_seq[-1]:\n",
    "                    if self.origin_data[j][1] not in tag_dict.keys():\n",
    "                        tag_dict[self.origin_data[j][1]]=1\n",
    "                    else:\n",
    "                        tag_dict[self.origin_data[j][1]]+=1\n",
    "            mi_value=0\n",
    "            tag=''\n",
    "            for key,value in tag_dict.items():\n",
    "                if value>mi_value:\n",
    "                    tag=key\n",
    "                    mi_value=value\n",
    "            tag_seq.append(tag)\n",
    "        feature_martix=np.zeros((len(tag_seq),300000))\n",
    "        for i in range(len(tag_seq)-1):\n",
    "            ind=0\n",
    "            for j in range(len(self.origin_data)):\n",
    "                if self.origin_data[j][0]==tag_seq[i]:\n",
    "                    ind=j\n",
    "                    break\n",
    "                if self.origin_data[j][1]==tag_seq[i]:\n",
    "                    ind=j\n",
    "                    break\n",
    "            feature_martix[i+1][ind]=1\n",
    "        return feature_martix,tag_seq\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    # BONUS (not required)\n",
    "    def predict_viterbi(self, tokens: TokenSeq) -> Tuple[NDArray, NDArray, TagSeq]:\n",
    "        \"\"\"\n",
    "        Predicts part-of-speech tags for the sequence of tokens using the\n",
    "        Viterbi algorithm, and returns the transition probability tensor,\n",
    "        the Viterbi lattice, and the predicted tags.\n",
    "\n",
    "        The entry i,j,k in the transition probability tensor should correspond\n",
    "        to the log-probability estimated by the classifier of token i having\n",
    "        part-of-speech tag k, given that the previous part-of-speech tag was j.\n",
    "        Thus, the first dimension should match the number of tokens, the second\n",
    "        dimension should be one more than the number of part of speech tags (the\n",
    "        last entry in this dimension corresponds to \"<s>\"), and the third\n",
    "        dimension should match the number of part-of-speech tags.\n",
    "\n",
    "        The entry i,k in the Viterbi lattice should correspond to the maximum\n",
    "        log-probability achievable via any path from token 0 to token i and\n",
    "        ending at assigning token i the part-of-speech tag k.\n",
    "\n",
    "        The predicted part-of-speech tags should correspond to the highest\n",
    "        probability path through the lattice.\n",
    "\n",
    "        :param tokens: A sequence of tokens representing a sentence.\n",
    "        \n",
    "        :return: The transition probability tensor, the Viterbi lattice, and the\n",
    "        sequence of predicted part-of-speech tags (one for each input token).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb0b0eda0c3f2dd2ba77c812ca6b55c5",
     "grade": false,
     "grade_id": "ptb-tags",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# part-of-speech tags from the Penn Treebank\n",
    "PTB_TAGS = {\n",
    "    \"#\", \"$\", \"''\", \"``\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"CC\", \"CD\", \"DT\",\n",
    "    \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNP\", \"NNPS\",\n",
    "    \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\",\n",
    "    \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d72a64679957dba3c3774e94eb42c81",
     "grade": false,
     "grade_id": "md-test-read-data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test `.read_ptbtagged()` (3 pts)\n",
    "\n",
    "Tests that you read in a) the correct number of sentences and tokens from the training data, b) that all `PTB_TAGS` were found in that partition of the data, and c) each token has exactly one corresponding tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22de0cc79b92c99147f03b35deec9a8a",
     "grade": true,
     "grade_id": "test-read-data",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_read_ptbtagged():\n",
    "    # keep a counter here (instead of enumerate) in case the iterator is empty\n",
    "    token_count = 0\n",
    "    sentence_count = 0\n",
    "    for sentence in read_ptbtagged(\"data/PTBSmall/train.tagged\"):\n",
    "        assert len(sentence) == 2\n",
    "        tokens, pos_tags = sentence\n",
    "        assert len(tokens) == len(pos_tags)\n",
    "        assert all(pos in PTB_TAGS for pos in pos_tags)\n",
    "        token_count += len(tokens)\n",
    "        sentence_count += 1\n",
    "    assert token_count == 191969\n",
    "    assert sentence_count == 8020\n",
    "\n",
    "    # check the sentence count in the dev set too\n",
    "    assert sum(1 for _ in read_ptbtagged(\"data/PTBSmall/dev.tagged\")) == 5039\n",
    "    \n",
    "test_read_ptbtagged()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fcfce5931f1fa947cfdb7a437446850",
     "grade": false,
     "grade_id": "md-test-features",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test features (5 pts)\n",
    "\n",
    "This test ensures you are, per the definition of MEMM, minimally representing **token** and **prior tag** ($t_{i-1}$) features.  \n",
    "\n",
    "Use the special symbol `<s>` to represent the prior tag of the first token in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73bb9b1a88027c59213aa14af31a2276",
     "grade": true,
     "grade_id": "test-features",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_feature_vectors():\n",
    "    clf       = Classifier()\n",
    "    ptb_train = read_ptbtagged(\"data/PTBSmall/train.tagged\")\n",
    "    ptb_train = itertools.islice(ptb_train, 2)  # just the first 2 sentences\n",
    "    features_matrix, labels_vector = clf.train(ptb_train)\n",
    "    # num. tokens\n",
    "    assert features_matrix.shape[0] == 31\n",
    "    assert labels_vector.shape[0] == 31\n",
    "\n",
    "    # train.tagged starts with\n",
    "    # Pierre\tNNP\n",
    "    # Vinken\tNNP\n",
    "    # ,\t,\n",
    "    # 61\tCD\n",
    "    # years\tNNS\n",
    "    # old\tJJ\n",
    "    assert features_matrix[4, clf.feature_index(\"token=years\")] == 1\n",
    "    assert features_matrix[4, clf.feature_index(\"token=old\")] == 0\n",
    "    assert features_matrix[4, clf.feature_index(\"pos-1=CD\")] == 1\n",
    "    assert features_matrix[4, clf.feature_index(\"pos-1=NNS\")] == 0\n",
    "    assert features_matrix[0, clf.feature_index(\"pos-1=<s>\")] == 1\n",
    "    assert labels_vector[3] == clf.label_index(\"CD\")\n",
    "    assert labels_vector[4] == clf.label_index(\"NNS\")\n",
    "test_feature_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "354c6985b109e27dd20b3dbecfaff9a0",
     "grade": false,
     "grade_id": "md-test-greedy-decoding",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test greedy decoding (5pts)\n",
    "\n",
    "In the greedy decoding approach, each tag is predicted one at a time, and each prediction is considered a **hard** decision.  In other words, when predicting the tag for token $t_{i}$, the model will assume that its prediction for the prior token $t_{i-1}$ is correct and unchangeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fd83a94de32643a135021a5b341edec",
     "grade": true,
     "grade_id": "test-greedy-decoding",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_predict_greedy():\n",
    "    clf        = Classifier()\n",
    "    ptb_train  = read_ptbtagged(\"data/PTBSmall/train.tagged\")\n",
    "    ptb_train  = itertools.islice(ptb_train, 2)  # just the 1st 2 sentences\n",
    "    clf.train(ptb_train)\n",
    "\n",
    "    tokens = \"Vinken is a director .\".split()\n",
    "    features_matrix, pos_tags = clf.predict_greedy(tokens)\n",
    "\n",
    "    # check that there is one feature vector per POS tag\n",
    "    assert features_matrix.shape[0] == len(pos_tags)\n",
    "\n",
    "    # check that all POS tags are in the PTB tagset\n",
    "    assert all(pos_tag in PTB_TAGS for pos_tag in pos_tags)\n",
    "\n",
    "    def last_pos_index(ptb_tag):\n",
    "        return clf.feature_index(\"pos-1=\" + ptb_tag)\n",
    "\n",
    "    # check that the first word (\"The\") has no pos-1 feature\n",
    "    for ptb_tag in {\"NNP\", \",\", \"CD\", \"NNS\", \"JJ\", \"MD\", \"VB\", \"DT\", \"NN\", \"IN\",\n",
    "                    \"VBZ\", \"VBG\"}:\n",
    "        assert features_matrix[0, last_pos_index(ptb_tag)] == 0\n",
    "\n",
    "    # check that the remaining words have the correct pos-1 features\n",
    "    for i, pos_tag in enumerate(pos_tags[:-1]):\n",
    "        assert features_matrix[i + 1, last_pos_index(pos_tag)] > 0\n",
    "\n",
    "test_predict_greedy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b7192194e5d8e39c77a667a8915acfd",
     "grade": false,
     "grade_id": "md-min-min-accuracy",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Minimum accuracy (4pts)\n",
    "\n",
    "Your model should achieve >= 93% acccuracy against the first 100 sentences of the Penn Treebank development partition.  To achieve this accuracy, you may need to include additional contextual features (i.e., features that represent information about the surrounding words and/or tags).\n",
    "\n",
    "**WARNING**: _this test may be slow to run (2 min.+)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cd5cab7ce4d9ceca31116153f7dbd69",
     "grade": true,
     "grade_id": "test-min-accuracy",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "94.0% accuracy on first 100 sentences of PTB dev\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy():\n",
    "    clf       = Classifier()\n",
    "    ptb_train = read_ptbtagged(\"data/PTBSmall/train.tagged\")\n",
    "    clf.train(ptb_train)\n",
    "\n",
    "    total_count   = 0\n",
    "    correct_count = 0\n",
    "    ptb_dev = read_ptbtagged(\"data/PTBSmall/dev.tagged\")\n",
    "    ptb_dev = itertools.islice(ptb_dev, 100)  # just the 1st 100 sentences\n",
    "    for tokens, pos_tags in ptb_dev:\n",
    "        total_count += len(tokens)\n",
    "        predicted_tags = clf.predict(tokens)\n",
    "        assert len(predicted_tags) == len(pos_tags)\n",
    "        for predicted_tag, true_tag in zip(predicted_tags, pos_tags):\n",
    "            if predicted_tag == true_tag:\n",
    "                correct_count += 1\n",
    "    accuracy = correct_count / total_count\n",
    "\n",
    "    # print out performance\n",
    "    sg = f\"\\n{accuracy:.1%} accuracy on first 100 sentences of PTB dev\"\n",
    "    print(sg)\n",
    "    \n",
    "    assert accuracy >= 0.93\n",
    "\n",
    "test_accuracy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
